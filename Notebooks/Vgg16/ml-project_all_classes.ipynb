{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7108648,"sourceType":"datasetVersion","datasetId":4098494},{"sourceId":7125582,"sourceType":"datasetVersion","datasetId":4110473}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install split-folders","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:36:51.525791Z","iopub.execute_input":"2023-12-05T03:36:51.526194Z","iopub.status.idle":"2023-12-05T03:37:04.378413Z","shell.execute_reply.started":"2023-12-05T03:36:51.526162Z","shell.execute_reply":"2023-12-05T03:37:04.377467Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting split-folders\n  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\nInstalling collected packages: split-folders\nSuccessfully installed split-folders-0.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision.transforms import transforms\nimport os\nfrom PIL import Image\nimport torch\nimport ssl\nimport torchvision\nimport torchvision.models as models\nimport torch.optim as optim\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport splitfolders\nimport csv\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\nimport numpy as np\n\n\ndef is_folder_empty(folder_path):\n    # Check if the folder exists\n    if not os.path.exists(folder_path):\n        print(f\"The folder '{folder_path}' does not exist.\")\n        return False\n\n    # Check if the folder is empty\n    for root, dirs, files in os.walk(folder_path):\n        if not dirs and not files:\n            print(f\"The folder '{root}' is empty.\")\n            return True  # The folder is empty\n\n    print(f\"The folder '{folder_path}' is not empty.\")\n    return False\n\ndef get_subdirectories(folder_path):\n    subdirectories = []\n    for entry in os.listdir(folder_path):\n        entry_path = os.path.join(folder_path, entry)\n        if os.path.isdir(entry_path):\n            subdirectories.append(entry_path)\n    return subdirectories\n\ndef calculate_accuracy(y_pred, y):\n    top_pred = y_pred.argmax(1, keepdim=True)\n    correct = top_pred.eq(y.view_as(top_pred)).sum()\n    acc = correct.float() / y.shape[0]\n    return acc\n\nssl._create_default_https_context = ssl._create_unverified_context\n\nif __name__ == '__main__':\n    # Define the device to be used for training\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Set up the transform to resize and normalize the images\n    transform = transforms.Compose([\n        transforms.Resize(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n            std=[0.5, 0.5, 0.5]),\n    ])\n\n    # Update input folder and output folder paths\n    input_folder = r\"/kaggle/input/images-highpassfilter-small-zip/IMAGES_HighPassFilter_Small\"\n    output_folder = r\"/kaggle/input/images-split-highpass/IMAGES_Split_HighPass\"\n\n    ### Uncomment only for first time. once data is splitted into train and validation, comment it out\n    #splitfolders.ratio(input_folder, output_folder, seed=42, ratio=(0.8, 0.2), group_prefix=None)\n\n    # Create datasets for the training and testing sets\n    train_dataset = torchvision.datasets.ImageFolder(output_folder + '/train', transform=transform)\n    val_dataset = torchvision.datasets.ImageFolder(output_folder + '/val', transform=transform)\n    train_size = len(train_dataset)\n    val_size = len(val_dataset)\n\n    # Create the data loaders for training and validation\n    train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True,num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=5, shuffle=True,num_workers=4)\n    list_of_classes = os.listdir(r\"/kaggle/input/images-split-highpass/IMAGES_Split_HighPass/train\")\n    print(list_of_classes)\n    classes = list(train_dataset.class_to_idx.keys())\n    classes.sort()\n\n\n    # Define the VGG model\n    model = torchvision.models.vgg16(pretrained=True)\n    num_features = model.classifier[0].in_features\n    model.classifier = nn.Sequential(\n        nn.Linear(num_features, 4096),\n        nn.ReLU(inplace=True),\n        nn.Dropout(0.5),\n        nn.Linear(4096, 4096),\n        nn.ReLU(inplace=True),\n        nn.Dropout(0.5),\n        nn.Linear(4096, len(list_of_classes))\n    )\n\n    ## uncomment for CPU\n    model = model.to(device)\n\n    # Define the loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    START_LR = 0.0001\n    optimizer = optim.Adam(model.parameters(), lr=START_LR)\n    model = model.to(device)\n    criterion = criterion.to(device)\n\n    ### uncomment for GPU\n    model.cuda()\n        # Train the model\n    for epoch in range(1):\n        ### uncomment for GPU\n        torch.cuda.empty_cache()\n        print('Epoch {}/{}'.format(epoch + 1, 1))\n        print('-' * 1)\n\n        running_loss = 0\n        running_corrects = 0\n\n        model.train()\n        predictions = []\n        true_labels = []\n        for inputs, labels in tqdm(train_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            true_labels.extend(labels.cpu().numpy())\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            predictions.extend(preds.cpu().numpy())\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n        epoch_loss = running_loss / train_size\n        epoch_acc = running_corrects.double() / train_size\n        report_dict = classification_report(true_labels, predictions, target_names=list_of_classes,output_dict=True)\n        report_pd = pd.DataFrame(report_dict)\n        report_pd.to_csv('training-classification-epoch' + str(epoch + 1) + '.csv')\n        cnf_matrix = confusion_matrix(true_labels, predictions)\n        df_cm = pd.DataFrame(cnf_matrix / np.sum(cnf_matrix, axis=1)[:, None], index = [i for i in classes],\n                        columns = [i for i in classes])\n        df_cm.to_csv('confusion-matrix-train-epoch' + str(epoch + 1) + '.csv')\n        #acc = matrix.diagonal()/matrix.sum(axis=1)\n        FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n        FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n        TP = np.diag(cnf_matrix)\n        TN = cnf_matrix.sum() - (FP + FN + TP)\n        FP = FP.astype(float)\n        FN = FN.astype(float)\n        TP = TP.astype(float)\n        TN = TN.astype(float)\n        ACC = (TP+TN)/(TP+FP+FN+TN)\n        TPR = TP/(TP+FN)\n        PPV = TP/(TP+FP)\n        print(\"accuracy for all classes in train phase\", ACC)\n        print(\"recall for all classes in train phase\", TPR)\n        print(\"precision for all classes in train phase\", PPV)\n        \n        pd.DataFrame(ACC, columns=['Accuracy']).to_csv('accuracy-train-epoch' + str(epoch + 1) + '.csv')\n        print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n        \n        # Delete data to clear GPU memory\n        del outputs\n        del preds\n        del labels\n        del inputs\n        torch.cuda.empty_cache()\n        \n        # Validation phase\n        running_loss = 0\n        running_corrects = 0\n        model.eval()  # set the model to evaluation mode\n        predictions = []\n        true_labels = []\n        for inputs, labels in tqdm(val_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            true_labels.extend(labels.cpu().numpy())\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            predictions.extend(preds.cpu().numpy())\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n        epoch_loss = running_loss / val_size\n        epoch_acc = running_corrects.double() / val_size\n       \n        #classification_report(true_labels, predictions, target_names=list_of_classes,output_dict=True)\n        report_dict = classification_report(true_labels, predictions, target_names=list_of_classes,output_dict=True)\n        report_pd = pd.DataFrame(report_dict)\n        report_pd.to_csv('val-classification-epoch' + str(epoch + 1) + '.csv')\n        #matrix = confusion_matrix(true_labels, predictions)\n        cnf_matrix = confusion_matrix(true_labels, predictions)\n        df_cm = pd.DataFrame(cnf_matrix / np.sum(cnf_matrix, axis=1)[:, None], index = [i for i in classes],\n                        columns = [i for i in classes])\n        df_cm.to_csv('confusion-matrix-val-epoch' + str(epoch + 1) + '.csv')\n        #acc = matrix.diagonal()/matrix.sum(axis=1)\n        FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n        FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n        TP = np.diag(cnf_matrix)\n        TN = cnf_matrix.sum() - (FP + FN + TP)\n        FP = FP.astype(float)\n        FN = FN.astype(float)\n        TP = TP.astype(float)\n        TN = TN.astype(float)\n        ACC = (TP+TN)/(TP+FP+FN+TN)\n        TPR = TP/(TP+FN)\n        PPV = TP/(TP+FP)\n        pd.DataFrame(ACC, columns=['Accuracy']).to_csv('accuracy-val-epoch' + str(epoch + 1) + '.csv')\n        print(\"accuracy for all classes in validation phase\", ACC)\n        print(\"recall for all classes in validation phase\", TPR)\n        print(\"precision for all classes in validation phase\", PPV)\n        print('Val Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n        \n        # Delete data to clear GPU memory\n        del outputs\n        del preds\n        del labels\n        del inputs\n        torch.cuda.empty_cache()\n                \n        \n\n    # Save the model\n    torch.save(model, 'vgg16_model.pth')\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-05T03:37:23.973323Z","iopub.execute_input":"2023-12-05T03:37:23.973694Z","iopub.status.idle":"2023-12-05T04:08:31.384777Z","shell.execute_reply.started":"2023-12-05T03:37:23.973663Z","shell.execute_reply":"2023-12-05T04:08:31.383726Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"['yetgre1', 'moccha1', 'rostur1', 'walsta1', 'ratcis1', 'norfis1', 'macshr1', 'brrwhe3', 'crefra2', 'pabspa1', 'sltnig1', 'cabgre1', 'equaka1', 'sobfly1', 'rindov', 'wlwwar', 'brwwar1', 'gnbcam2', 'carcha1', 'abethr1', 'yertin1', 'spewea1', 'varsun2', 'yebduc1', 'eubeat1', 'hadibi1', 'brcale1', 'litwea1', 'sincis1', 'whbcro2', 'thrnig1', 'bubwar2', 'kvbsun1', 'blbpuf2', 'blakit1', 'colsun2', 'bltapa1', 'gycwar3', 'joygre1', 'greegr', 'vibsta2', 'wtbeat1', 'afrgos1', 'rebfir2', 'yebgre1', 'comsan', 'pygbat1', 'meypar1', 'yelbis1', 'norbro1', 'ndcsun2', 'gybfis1', 'reftin1', 'brobab1', 'refwar2', 'norcro1', 'yebapa1', 'yewgre1', 'palfly2', 'gargan', 'darter3', 'rerswa1', 'augbuz1', 'gyhbus1', 'refcro1', 'witswa1', 'gryapa1', 'pitwhy', 'eaywag1', 'blhgon1', 'yebsto1', 'hipbab1', 'whcpri2', 'spemou2', 'gobsta5', 'blksaw1', 'afecuc1', 'spepig1', 'mabeat1', 'rewsta1', 'rebhor1', 'brtcha1', 'blacuc1', 'brican1', 'rehblu1', 'gobbun1', 'supsta1', 'bkfruw1', 'litswi1', 'spmthr1', 'spwlap1', 'quailf1', 'golher1', 'didcuc1', 'gytbar1', 'klacuc1', 'afbfly1', 'brcsta1', 'bawhor2', 'whihel1', 'yespet1', 'dotbar1', 'luebus1', 'yeccan1', 'tafpri1', 'chespa1', 'blacra1', 'scthon1', 'whbcou1', 'ccbeat1', 'libeat1', 'whctur2', 'butapa1', 'norpuf1', 'blwlap1', 'afmdov1', 'hartur1', 'beasun2', 'vimwea1', 'squher1', 'yebbar1', 'bltori1', 'sccsun2', 'piecro1', 'chibat1', 'marsto1', 'afpfly1', 'bcbeat1', 'wbswea1', 'yebere1', 'rbsrob1', 'brcwea1', 'bswdov1', 'kerspa2', 'slcbou1', 'fislov1', 'cohmar1', 'lesmaw1', 'cibwar1', 'woosan', 'shesta1', 'reccor', 'gnhsun1', 'chucis1', 'fatrav1', 'slbgre1', 'afghor1', 'afrjac1', 'abhori1', 'wbgbir1', 'subbus1', 'bawman1', 'whrshr1', 'hoopoe', 'lessts1', 'rocmar2', 'lotlap1', 'tamdov1', 'rufcha2', 'palpri1', 'reboxp1', 'chewea1', 'malkin1', 'vilwea1', 'reccuc1', 'bltbar1', 'trobou1', 'abythr1', 'broman1', 'easmog1', 'spfbar1', 'afpwag1', 'refbar2', 'strher', 'whhsaw1', 'grbcam1', 'sichor1', 'crheag1', 'wookin1', 'helgui', 'strsee1', 'chtapa3', 'grccra1', 'brubru1', 'wbrcha2', 'bkctch1', 'yesbar1', 'scrcha1', 'affeag1', 'grwpyt1', 'whbtit5', 'spfwea1', 'brosun1', 'combuz1', 'tacsun1', 'darbar1', 'grewoo2', 'purgre2', 'grecor', 'whbcan1', 'afrgrp1', 'mouwag1', 'bagwea1', 'eswdov1', 'blfbus1', 'soucit1', 'blnmou1', 'gbesta1', 'whbwhe3', 'somgre1', 'afrthr1', 'carwoo1', 'yenspu1', 'gobwea1', 'wfbeat1', 'blnwea1', 'soufis1', 'hunsun2', 'nobfly1', 'gyhkin1', 'nubwoo1', 'afpkin1', 'marsun2', 'gabgos2', 'yefcan', 'btweye2', 'huncis1', 'raybar1', 'dutdov1', 'gyhneg1', 'stusta1', 'wheslf1', 'somtit4', 'mcptit1', 'whbwea1', 'lawgol', 'combul2', 'gyhspa1', 'ruegls1', 'fotdro5', 'afdfly1', 'sacibi2', 'hamerk1', 'piekin1', 'afgfly1', 'reisee2', 'amesun2', 'laudov1', 'grywrw1', 'blhher1', 'loceag1', 'crohor1', 'lotcor1', 'brctch1', 'barswa', 'categr', 'reedov1', 'blaplo1', 'litegr', 'egygoo', 'rehwea1', 'fatwid1', 'blcapa2', 'edcsun3']\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:01<00:00, 315MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1\n-\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 18324/18324 [27:56<00:00, 10.93it/s]\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/tmp/ipykernel_47/588170330.py:156: RuntimeWarning: invalid value encountered in divide\n  PPV = TP/(TP+FP)\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.9994106  0.9889978  0.99813355 0.99917047 0.99861381 0.99460805\n 0.99907223 0.9997817  0.995896   0.99745683 0.99481543 0.99998909\n 0.99371303 0.99621253 0.9992687  0.99899583 0.99377852 0.99764238\n 0.99977079 0.9991923  0.9594512  0.99763147 0.9997817  0.99700932\n 0.99783885 0.99407322 0.99835185 0.99704207 0.99548124 0.98271082\n 0.9977297  0.9986793  0.99898492 0.99746775 0.99884302 0.99945426\n 0.99982536 0.99845009 0.99934511 0.999487   0.99975987 0.99923596\n 0.99963981 0.99951975 0.99973804 0.99762055 0.99994543 0.99860289\n 0.99841734 0.99849375 0.99893034 0.99951975 0.99995634 0.99647449\n 0.99788251 0.99860289 0.99541575 0.99969438 0.99822087 0.998461\n 0.98763343 0.99703115 0.97808291 0.9992687  0.99974896 0.99965072\n 0.99549215 0.99873387 0.99903949 0.9936912  0.95738829 0.98610535\n 0.95964767 0.96928551 0.99008928 0.99993451 0.99843917 0.99660547\n 0.99799166 0.99987994 0.99588509 0.99995634 0.99972713 0.99947609\n 0.97657666 0.99917047 0.99050405 0.99975987 0.99681285 0.97237442\n 0.99922504 0.99986902 0.99955249 0.97647842 0.99705298 0.99638717\n 0.99717304 0.97748259 0.99894126 0.99432426 0.99985811 0.9997053\n 0.99998909 0.99421511 0.99932328 0.97773363 0.98879041 0.99255605\n 0.99909406 0.99874479 0.99908315 0.99938877 0.99711847 0.99417145\n 0.99811172 0.99945426 0.99647449 0.99974896 0.99298173 0.99923596\n 0.99931236 0.99644175 0.99936694 0.90197341 0.99944334 0.99971621\n 0.99980353 0.99764238 0.99690017 0.99967255 0.99377852 0.98622541\n 0.99903949 0.99859198 0.99917047 0.97958916 0.99003471 0.999487\n 0.99903949 0.99995634 0.99998909 0.99900675 0.99921413 0.99945426\n 0.99960706 0.99973804 0.99789343 0.9995634  0.99847192 0.99616888\n 0.99792617 0.99877753 0.99845009 0.99895217 0.99830819 0.99906132\n 0.99895217 0.999487   0.99972713 0.99949792 0.99984719 0.99609247\n 0.996409   0.99739134 0.99975987 0.99935602 0.99887577 0.98710952\n 0.99937785 0.97139208 0.9978716  0.99836277 0.99915955 0.99759873\n 0.99243598 0.99690017 0.99954157 0.99833002 0.99750049 0.99954157\n 0.99996726 0.99969438 0.99911589 0.9846755  0.99863564 0.99452073\n 0.99877753 0.9997817  0.99726036 0.99981445 0.99983628 0.9961907\n 0.99793709 0.99884302 0.99977079 0.99779519 0.99736951 0.99901766\n 0.99768604 0.99823179 0.99896309 0.97799559 0.99917047 0.99890851\n 0.99607064 0.99853741 0.99807898 0.99925779 0.99943243 0.9997053\n 0.99385492 0.99732585 0.99923596 0.99777336 0.9983846  0.9997817\n 0.99531751 0.99854832 0.99973804 0.9889978  0.98454452 0.83749918\n 0.99467354 0.99198847 0.99920321 0.99111528 0.99903949 0.99767513\n 0.99888668 0.98581065 0.99511013 0.99857015 0.99950883 0.997435\n 0.99979262 0.99921413 0.99943243 0.99836277 0.9995634  0.99998909\n 0.99925779 0.99998909 0.99937785 0.99975987 0.99986902 0.93441245\n 0.99616888 0.98104084 0.99899583 0.99503373 0.99781702 0.99949792\n 0.99817721 0.99878845 0.99991268 0.99934511 0.99524111 0.99920321\n 0.99971621 0.97948001 0.99834094 0.99947609 0.99939968 0.99022026]\nrecall for all classes in train phase [0.         0.31806931 0.         0.         0.01652893 0.4541387\n 0.01219512 0.         0.17628205 0.         0.         0.\n 0.14247312 0.02719033 0.         0.02197802 0.29004329 0.\n 0.         0.         0.35478615 0.01       0.         0.24793388\n 0.01104972 0.07231405 0.11627907 0.00373134 0.36692506 0.21768707\n 0.11675127 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.04166667\n 0.01630435 0.         0.07506702 0.         0.02739726 0.\n 0.35344015 0.         0.31428571 0.         0.         0.\n 0.07073171 0.         0.         0.02052239 0.57461538 0.17671518\n 0.42043121 0.57817014 0.584      0.         0.01428571 0.03225806\n 0.18918919 0.         0.103125   0.         0.         0.\n 0.58635704 0.         0.1231454  0.         0.         0.79341685\n 0.         0.         0.         0.20542636 0.05932203 0.\n 0.00396825 0.3466788  0.         0.04481132 0.         0.\n 0.         0.01367188 0.         0.26977153 0.11985019 0.06921676\n 0.         0.05825243 0.         0.         0.00389105 0.00193798\n 0.         0.         0.         0.         0.22626263 0.\n 0.         0.01582278 0.         0.3844367  0.         0.\n 0.         0.03645833 0.00363636 0.         0.00177936 0.33231397\n 0.         0.         0.         0.3424     0.38198758 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.00287356\n 0.06024096 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.02108434\n 0.         0.00847458 0.         0.         0.00961538 0.16553596\n 0.         0.54894895 0.         0.         0.         0.10555556\n 0.48387097 0.         0.         0.         0.         0.\n 0.         0.         0.         0.27678571 0.00819672 0.00200401\n 0.         0.         0.00409836 0.         0.         0.00295858\n 0.06060606 0.         0.         0.         0.         0.\n 0.         0.40782123 0.         0.41368078 0.         0.03125\n 0.06690141 0.         0.         0.         0.         0.\n 0.02929688 0.15021459 0.         0.         0.         0.\n 0.03714286 0.         0.         0.183391   0.3        0.87369117\n 0.00423729 0.14666667 0.         0.12734082 0.         0.00930233\n 0.         0.19361936 0.01485149 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.61122472\n 0.12171053 0.41707921 0.         0.17105263 0.01111111 0.\n 0.         0.         0.         0.         0.02203857 0.\n 0.         0.30243161 0.03333333 0.         0.         0.17132867]\nprecision for all classes in train phase [       nan 0.35994398        nan        nan 0.2        0.44812362\n 0.2               nan 0.31609195 0.         0.                nan\n 0.17096774 0.26470588        nan 0.4        0.35638298 0.\n        nan 0.         0.29013991 0.0952381         nan 0.39473684\n 0.0952381  0.27131783 0.28846154 0.2        0.45659164 0.34408602\n 0.40350877        nan 0.         0.                nan        nan\n        nan 0.                nan 0.                nan        nan\n        nan        nan        nan 0.                nan 0.\n        nan        nan        nan        nan        nan 0.35135135\n 0.1875            nan 0.27184466        nan 0.16       0.\n 0.45620438        nan 0.18734793        nan        nan        nan\n 0.47540984 0.                nan 0.171875   0.34808947 0.26113671\n 0.24180691 0.54559224 0.75517241        nan 0.28571429 0.47619048\n 0.30434783        nan 0.26829268        nan        nan        nan\n 0.48421894        nan 0.22928177        nan        nan 0.65650407\n        nan        nan        nan 0.13703943 0.22580645 0.\n 0.11111111 0.2207416         nan 0.14179104        nan        nan\n        nan 0.21875           nan 0.2025066  0.22966507 0.18181818\n        nan 0.25              nan        nan 0.11111111 0.05\n        nan        nan 0.                nan 0.30107527        nan\n        nan 0.25              nan 0.16175932        nan        nan\n        nan 0.18421053 0.09090909        nan 0.1        0.3494105\n        nan        nan        nan 0.2899729  0.32325887        nan\n 0.                nan        nan        nan        nan        nan\n        nan        nan 0.                nan        nan 0.2\n 0.22727273        nan        nan        nan 0.                nan\n        nan        nan        nan        nan        nan 0.175\n        nan 0.28571429 0.                nan 1.         0.17732558\n        nan 0.3283046         nan 0.                nan 0.24358974\n 0.44576523        nan        nan        nan 0.                nan\n        nan        nan 0.         0.29245283 0.2        0.2\n        nan        nan 0.11111111        nan        nan 0.07692308\n 0.22727273 0.                nan 0.         0.                nan\n 0.         0.56589147 0.         0.28159645        nan 0.3\n 0.16666667        nan        nan        nan        nan        nan\n 0.18518519 0.42682927        nan        nan 0.                nan\n 0.12380952 0.                nan 0.16510903 0.19148936 0.36582214\n 0.1        0.21212121        nan 0.16346154        nan 1.\n        nan 0.23687752 0.10714286 0.         0.                nan\n        nan        nan        nan 0.                nan        nan\n        nan        nan        nan        nan        nan 0.38494562\n 0.30578512 0.45881552 0.         0.31707317 0.08333333        nan\n        nan        nan        nan        nan 0.08988764        nan\n        nan 0.12283951 0.41666667        nan        nan 0.18846154]\nTrain Loss: 3.2342 Acc: 0.3374\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4607/4607 [02:34<00:00, 29.86it/s]\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/tmp/ipykernel_47/588170330.py:210: RuntimeWarning: invalid value encountered in divide\n  PPV = TP/(TP+FP)\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in validation phase [0.9993922  0.98411045 0.99813319 0.99917513 0.99869758 0.99839368\n 0.99904489 0.99973952 0.9972215  0.99748198 0.99457324 0.99995659\n 0.99535469 0.99622297 0.99926196 0.99891465 0.9969176  0.99765564\n 0.99973952 0.99917513 0.96418338 0.99769905 0.99973952 0.9966137\n 0.9978293  0.9938352  0.99887123 0.99696101 0.99561518 0.97004428\n 0.99791612 0.99865416 0.99904489 0.99756881 0.99882782 0.99943562\n 0.99982634 0.99848051 0.99930537 0.99956586 0.99973952 0.99921855\n 0.99960927 0.99947903 0.99973952 0.99761223 0.99991317 0.99861075\n 0.99839368 0.99848051 0.99891465 0.99952245 0.99995659 0.99526786\n 0.99822002 0.99856734 0.99683077 0.9996961  0.99826344 0.99848051\n 0.98918989 0.99700443 0.97429886 0.99926196 0.99973952 0.99965269\n 0.99626639 0.99878441 0.99904489 0.99400886 0.979031   0.98246071\n 0.97937831 0.97069549 0.99522445 0.99991317 0.99765564 0.99652687\n 0.99795954 0.99986976 0.99266302 0.99995659 0.9996961  0.99947903\n 0.99092646 0.99913172 0.99274985 0.99973952 0.99670053 0.99396544\n 0.99921855 0.99986976 0.99952245 0.95450204 0.99604932 0.9963098\n 0.99696101 0.98419727 0.99891465 0.99296692 0.99982634 0.9996961\n 0.99995659 0.99448641 0.99930537 0.97677347 0.99140401 0.98853868\n 0.9990883  0.99900148 0.99904489 0.9993922  0.99717808 0.9938352\n 0.99808978 0.99943562 0.99531128 0.99973952 0.9929235  0.99921855\n 0.99930537 0.99544152 0.99934879 0.94976991 0.99943562 0.9996961\n 0.99978293 0.99244595 0.99696101 0.99965269 0.99392203 0.9876704\n 0.99900148 0.99856734 0.99917513 0.97468959 0.9960059  0.99947903\n 0.99904489 0.99991317 0.99995659 0.99900148 0.99921855 0.99943562\n 0.99960927 0.99973952 0.9978293  0.99956586 0.99843709 0.9960059\n 0.99830685 0.99878441 0.99839368 0.99895806 0.99822002 0.99904489\n 0.99895806 0.99947903 0.9996961  0.99947903 0.99982634 0.99626639\n 0.99596249 0.99743857 0.99973952 0.99943562 0.99882782 0.98098463\n 0.99934879 0.98289485 0.99778588 0.99835026 0.99913172 0.98814796\n 0.99756881 0.99678736 0.99952245 0.99830685 0.9975254  0.99952245\n 0.99995659 0.9996961  0.99895806 0.97686029 0.99865416 0.99457324\n 0.99878441 0.99978293 0.99644005 0.99978293 0.99982634 0.99487714\n 0.99826344 0.99878441 0.99973952 0.99774247 0.99730833 0.99900148\n 0.99765564 0.99930537 0.99887123 0.98636798 0.99917513 0.99874099\n 0.99083963 0.99852392 0.99804637 0.99926196 0.99943562 0.9996961\n 0.99418251 0.99878441 0.99921855 0.9975254  0.99839368 0.99978293\n 0.99296692 0.99856734 0.9996961  0.99092646 0.99031866 0.97881393\n 0.99466007 0.99283668 0.99917513 0.98315534 0.99904489 0.9975254\n 0.99887123 0.98502214 0.99526786 0.99856734 0.99952245 0.99743857\n 0.99978293 0.99917513 0.9993922  0.99817661 0.99956586 0.99995659\n 0.99926196 0.99995659 0.99934879 0.99973952 0.99982634 0.9808978\n 0.99596249 0.99257619 0.99891465 0.99539811 0.99739515 0.99947903\n 0.99822002 0.99882782 0.99991317 0.99934879 0.99140401 0.99917513\n 0.9996961  0.99057914 0.99800295 0.99943562 0.9993922  0.99344447]\nrecall for all classes in validation phase [0.         0.75742574 0.         0.         0.03225806 0.83035714\n 0.         0.         0.26923077 0.         0.02564103 0.\n 0.13829787 0.19277108 0.         0.         0.68965517 0.\n 0.         0.         0.61237785 0.04       0.         0.70491803\n 0.10869565 0.37190083 0.24242424 0.01470588 0.77319588 0.52265861\n 0.62       0.         0.04347826 0.12068966 0.         0.\n 0.         0.         0.         0.16666667 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.12820513\n 0.14893617 0.         0.24468085 0.         0.10810811 0.\n 0.81954887 0.         0.6300813  0.         0.         0.\n 0.3592233  0.         0.         0.08148148 0.74347158 0.43153527\n 0.60860656 0.83311938 0.90691489 0.         0.47222222 0.03846154\n 0.63157895 0.         0.50617284 0.         0.         0.\n 0.84332689 0.         0.21893491 0.         0.         0.93123772\n 0.         0.         0.         0.64341085 0.6779661  0.01204819\n 0.         0.49818182 0.         0.41121495 0.         0.\n 0.         0.0078125  0.         0.47017544 0.30348259 0.28985507\n 0.         0.11538462 0.04545455 0.         0.         0.01550388\n 0.         0.         0.02564103 0.         0.53225806 0.\n 0.         0.15189873 0.         0.29732869 0.         0.\n 0.         0.32653061 0.         0.         0.0070922  0.5203252\n 0.         0.         0.         0.65175719 0.82098765 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.73809524 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.04819277\n 0.04819277 0.         0.         0.53333333 0.         0.56216216\n 0.         0.81534772 0.         0.         0.         0.6\n 0.76774194 0.         0.         0.         0.         0.\n 0.         0.         0.0952381  0.66666667 0.         0.\n 0.         0.         0.01612903 0.         0.         0.11764706\n 0.19047619 0.         0.         0.         0.         0.\n 0.         0.8        0.         0.61237785 0.         0.28\n 0.36111111 0.         0.         0.         0.         0.\n 0.0078125  0.59322034 0.         0.         0.         0.\n 0.29545455 0.         0.         0.39310345 0.4640884  0.85147642\n 0.04201681 0.31818182 0.         0.52985075 0.         0.05555556\n 0.         0.56140351 0.01980198 0.         0.         0.\n 0.         0.         0.         0.18918919 0.         0.\n 0.         0.         0.         0.         0.         0.7311535\n 0.35064935 0.66831683 0.08695652 0.4375     0.         0.\n 0.02380952 0.03571429 0.         0.         0.34065934 0.\n 0.         0.29090909 0.15789474 0.         0.         0.43055556]\nprecision for all classes in validation phase [       nan 0.32553191        nan        nan 1.         0.83783784\n 0.                nan 0.75              nan 0.21428571        nan\n 0.33333333 0.44444444        nan 0.         0.69565217        nan\n        nan        nan 0.39044652 0.28571429        nan 0.41747573\n 0.35714286 0.40540541 0.88888889 0.25       0.48701299 0.24539007\n 0.51666667        nan 1.         0.58333333        nan        nan\n        nan        nan        nan 1.                nan        nan\n        nan        nan        nan        nan        nan        nan\n        nan        nan        nan        nan        nan 0.19607843\n 0.875             nan 0.92              nan 0.36363636        nan\n 0.5202864         nan 0.23628049        nan        nan        nan\n 0.64912281        nan        nan 0.44       0.605      0.28032345\n 0.51118761 0.54355109 0.81971154        nan 0.32692308 0.375\n 0.42105263        nan 0.24117647        nan        nan        nan\n 0.77304965        nan 0.51388889        nan 0.         0.93215339\n        nan        nan        nan 0.14795009 0.35714286 0.25\n 0.         0.37741047        nan 0.30769231        nan        nan\n        nan 1.                nan 0.25868726 0.51260504 0.19417476\n        nan 1.         0.5               nan        nan 0.11764706\n        nan        nan 0.05882353        nan 0.38596491        nan\n        nan 0.24              nan 0.31683168        nan        nan\n        nan 0.10191083 0.                nan 1.         0.43537415\n        nan        nan        nan 0.30088496 0.67857143        nan\n        nan        nan        nan        nan        nan        nan\n        nan        nan 0.                nan        nan 0.\n 0.52542373        nan 0.                nan 0.                nan\n        nan        nan        nan        nan        nan 0.36363636\n 0.22222222        nan        nan 0.57142857 0.         0.22559653\n        nan 0.51750381 0.                nan        nan 0.09574468\n 0.85611511 0.                nan        nan        nan        nan\n        nan        nan 0.28571429 0.27228525        nan        nan\n        nan        nan 0.04545455        nan        nan 0.18867925\n 0.57142857 0.                nan 0.         0.                nan\n 0.         0.8372093  0.         0.49086162        nan 0.38888889\n 0.13612565        nan        nan        nan        nan        nan\n 0.125      0.8974359         nan 0.                nan        nan\n 0.20634921        nan        nan 0.32022472 0.4        0.9275084\n 0.35714286 0.35897436        nan 0.17929293        nan 0.33333333\n        nan 0.34316354 0.16666667        nan        nan        nan\n        nan        nan        nan 0.36842105        nan        nan\n        nan        nan        nan        nan        nan 0.84826133\n 0.38571429 0.87947883 0.33333333 0.44680851 0.                nan\n 1.         1.                nan        nan 0.18343195        nan\n        nan 0.32432432 0.3               nan        nan 0.47328244]\nVal Loss: 2.3845 Acc: 0.4868\n","output_type":"stream"}]}]}