{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7118798,"sourceType":"datasetVersion","datasetId":4105717}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install split-folders","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-06T02:12:26.347558Z","iopub.execute_input":"2023-12-06T02:12:26.347843Z","iopub.status.idle":"2023-12-06T02:12:39.608898Z","shell.execute_reply.started":"2023-12-06T02:12:26.347816Z","shell.execute_reply":"2023-12-06T02:12:39.607755Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting split-folders\n  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\nInstalling collected packages: split-folders\nSuccessfully installed split-folders-0.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install efficientnet_pytorch","metadata":{"execution":{"iopub.status.busy":"2023-12-06T02:13:28.620836Z","iopub.execute_input":"2023-12-06T02:13:28.621202Z","iopub.status.idle":"2023-12-06T02:13:42.527905Z","shell.execute_reply.started":"2023-12-06T02:13:28.621171Z","shell.execute_reply":"2023-12-06T02:13:42.526729Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting efficientnet_pytorch\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet_pytorch) (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\nBuilding wheels for collected packages: efficientnet_pytorch\n  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=925dc5517cf0881077c70cff4771b7961392609c422303d9d95130944c388da3\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\nSuccessfully built efficientnet_pytorch\nInstalling collected packages: efficientnet_pytorch\nSuccessfully installed efficientnet_pytorch-0.7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision.transforms import transforms\nimport os\nfrom PIL import Image\nimport torch\nimport ssl\nimport torchvision\nimport torchvision.models as models\nimport torch.optim as optim\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport splitfolders\nimport csv\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\nimport numpy as np\nfrom efficientnet_pytorch import EfficientNet\nimport warnings\n\nwarnings.filterwarnings(\n    action='ignore')\n\n\ndef calculate_accuracy(y_pred, y):\n    top_pred = y_pred.argmax(1, keepdim=True)\n    correct = top_pred.eq(y.view_as(top_pred)).sum()\n    acc = correct.float() / y.shape[0]\n    return acc\n\nssl._create_default_https_context = ssl._create_unverified_context\n\nif __name__ == '__main__':\n    # Define the device to be used for training\n    device = torch.device(\"cpu\")\n\n    # Set up the transform to resize and normalize the images\n    transform = transforms.Compose([\n        transforms.Resize(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n            std=[0.5, 0.5, 0.5]),\n    ])\n     \n   # loading the temp.zip and creating a zip object \n  # with ZipFile(\"C:\\\\Users\\\\sai mohan pulamolu\\\\\\ \n   #         Desktop\\\\geeks_dir\\\\temp\\\\temp.zip\", 'r') as zObject: \n  \n    # Extracting all the members of the zip  \n    # into a specific location. \n   # zObject.extractall( \n    #    path=\"C:\\\\Users\\\\sai mohan pulamolu\\\\Desktop\\\\geeks_dir\\\\temp\") \n    # Update input folder and output folder paths\n    input_folder = r\"/kaggle/input/dataset-small/IMAGES_HighPassFilter_Small\"\n    output_folder = r\"/kaggle/working/images_30_output\"\n    \n\n    ### Uncomment only for first time. once data is splitted into train and validation, comment it out\n    splitfolders.ratio(input_folder, output_folder, seed=42, ratio=(0.8, 0.2), group_prefix=None)\n\n    # Create datasets for the training and testing sets\n    train_dataset = torchvision.datasets.ImageFolder(output_folder + '/train', transform=transform)\n    val_dataset = torchvision.datasets.ImageFolder(output_folder + '/val', transform=transform)\n    train_size = len(train_dataset)\n    val_size = len(val_dataset)\n\n    # Create the data loaders for training and validation\n    train_loader = DataLoader(train_dataset, batch_size=25, shuffle=True,num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=25, shuffle=True,num_workers=4)\n    list_of_classes = os.listdir(r\"/kaggle/working/images_30_output/train\")\n    print(list_of_classes)\n    classes = list(train_dataset.class_to_idx.keys())\n    classes.sort()\n\n\n    # Define the VGG model\n    model_name = 'efficientnet-b0'  # You can change this to other versions (b1, b2, etc.)\n    model = EfficientNet.from_pretrained(model_name)\n    num_features = model._fc.in_features\n    model._fc = nn.Linear(num_features, len(list_of_classes))\n\n  \n\n    ## uncomment for CPU\n    model = model.to(device)\n\n    # Define the loss function and optimizer\n    \n    \n    criterion = nn.CrossEntropyLoss()\n    START_LR = 0.0001\n    optimizer = optim.Adam(model.parameters(), lr=START_LR)\n    model = model.to(device)\n    criterion = criterion.to(device)\n\n    ### uncomment for GPU\n    #model.cuda()\n        # Train the model\n    for epoch in range(10):\n        ### uncomment for GPU\n        #torch.cuda.empty_cache()\n        print('Epoch {}/{}'.format(epoch + 1, 10))\n        print('-' * 10)\n\n        running_loss = 0\n        running_corrects = 0\n\n        model.train()\n        predictions = []\n        true_labels = []\n        for inputs, labels in tqdm(train_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            true_labels.extend(labels.cpu().numpy())\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            predictions.extend(preds.cpu().numpy())\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n        epoch_loss = running_loss / train_size\n        epoch_acc = running_corrects.double() / train_size\n        report_dict = classification_report(true_labels, predictions, target_names=list_of_classes,output_dict=True)\n        report_pd = pd.DataFrame(report_dict)\n        report_pd.to_csv('training-classification-epoch' + str(epoch + 1) + '.csv')\n        cnf_matrix = confusion_matrix(true_labels, predictions)\n        df_cm = pd.DataFrame(cnf_matrix / np.sum(cnf_matrix, axis=1)[:, None], index = [i for i in classes],\n                        columns = [i for i in classes])\n        df_cm.to_csv('confusion-matrix-train-epoch' + str(epoch + 1) + '.csv')\n        #acc = matrix.diagonal()/matrix.sum(axis=1)\n        FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n        FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n        TP = np.diag(cnf_matrix)\n        TN = cnf_matrix.sum() - (FP + FN + TP)\n        FP = FP.astype(float)\n        FN = FN.astype(float)\n        TP = TP.astype(float)\n        TN = TN.astype(float)\n        ACC = (TP+TN)/(TP+FP+FN+TN)\n        TPR = TP/(TP+FN)\n        PPV = TP/(TP+FP)\n        print(\"accuracy for all classes in train phase\", ACC)\n        print(\"recall for all classes in train phase\", TPR)\n        print(\"precision for all classes in train phase\", PPV)\n        \n        pd.DataFrame(ACC, columns=['Accuracy']).to_csv('accuracy-train-epoch' + str(epoch + 1) + '.csv')\n        print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n        \n        # Delete data to clear GPU memory\n        del outputs\n        del preds\n        del labels\n        del inputs\n        torch.cuda.empty_cache()\n        \n        # Validation phase\n        running_loss = 0\n        running_corrects = 0\n        model.eval()  # set the model to evaluation mode\n        predictions = []\n        true_labels = []\n        for inputs, labels in tqdm(val_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            true_labels.extend(labels.cpu().numpy())\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            predictions.extend(preds.cpu().numpy())\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n        epoch_loss = running_loss / val_size\n        epoch_acc = running_corrects.double() / val_size\n       \n        #classification_report(true_labels, predictions, target_names=list_of_classes,output_dict=True)\n        report_dict = classification_report(true_labels, predictions, target_names=list_of_classes,output_dict=True)\n        report_pd = pd.DataFrame(report_dict)\n        report_pd.to_csv('val-classification-epoch' + str(epoch + 1) + '.csv')\n        #matrix = confusion_matrix(true_labels, predictions)\n        cnf_matrix = confusion_matrix(true_labels, predictions)\n        df_cm = pd.DataFrame(cnf_matrix / np.sum(cnf_matrix, axis=1)[:, None], index = [i for i in classes],\n                        columns = [i for i in classes])\n        df_cm.to_csv('confusion-matrix-val-epoch' + str(epoch + 1) + '.csv')\n        #acc = matrix.diagonal()/matrix.sum(axis=1)\n        FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n        FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n        TP = np.diag(cnf_matrix)\n        TN = cnf_matrix.sum() - (FP + FN + TP)\n        FP = FP.astype(float)\n        FN = FN.astype(float)\n        TP = TP.astype(float)\n        TN = TN.astype(float)\n        ACC = (TP+TN)/(TP+FP+FN+TN)\n        TPR = TP/(TP+FN)\n        PPV = TP/(TP+FP)\n        pd.DataFrame(ACC, columns=['Accuracy']).to_csv('accuracy-val-epoch' + str(epoch + 1) + '.csv')\n        # print(\"accuracy for all classes in validation phase\", ACC)\n        # print(\"recall for all classes in validation phase\", TPR)\n        # print(\"precision for all classes in validation phase\", PPV)\n        print('Val Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n        \n        # Delete data to clear GPU memory\n        del outputs\n        del preds\n        del labels\n        del inputs\n        torch.cuda.empty_cache()\n                \n        \n\n    # Save the model\n    torch.save(model, 'efficientnet_model.pth')","metadata":{"execution":{"iopub.status.busy":"2023-12-06T02:42:45.183472Z","iopub.execute_input":"2023-12-06T02:42:45.183887Z","iopub.status.idle":"2023-12-06T05:59:34.168965Z","shell.execute_reply.started":"2023-12-06T02:42:45.183851Z","shell.execute_reply":"2023-12-06T05:59:34.167807Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Copying files: 8689 files [00:09, 874.72 files/s] \nDownloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n","output_type":"stream"},{"name":"stdout","text":"['fislov1', 'stusta1', 'yebere1', 'mouwag1', 'scrcha1', 'gobbun1', 'vimwea1', 'chucis1', 'afrthr1', 'hunsun2', 'whbtit5', 'mabeat1', 'afrgrp1', 'fotdro5', 'whbwhe3', 'grbcam1', 'hadibi1', 'crheag1', 'varsun2', 'kvbsun1', 'grwpyt1', 'gbesta1', 'bkctch1', 'wtbeat1', 'squher1', 'blnwea1', 'pabspa1', 'categr', 'blhgon1', 'whbwea1']\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20.4M/20.4M [00:00<00:00, 56.0MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b0\nEpoch 1/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 278/278 [19:22<00:00,  4.18s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.98933564 0.9211702  0.92491714 0.98285055 0.9900562  0.85876928\n 0.98789451 0.98155354 0.9933708  0.77907479 0.96195417 0.91670269\n 0.93039343 0.9877504  0.94264303 0.99452371 0.99538838 0.98933564\n 0.98313878 0.99582072 0.97679781 0.98947975 0.99711774 0.92246721\n 0.98674161 0.98947975 0.99164145 0.97968007 0.98587693 0.97622136]\nrecall for all classes in train phase [0.         0.67532468 0.3553719  0.         0.         0.85612245\n 0.04545455 0.24285714 0.         0.67344961 0.08730159 0.45990566\n 0.32617188 0.         0.48484848 0.         0.         0.\n 0.36144578 0.         0.09090909 0.         0.         0.63619048\n 0.         0.         0.         0.19863014 0.         0.07185629]\nprecision for all classes in train phase [0.         0.44005642 0.45144357 0.         0.         0.5\n 1.         0.60714286 0.         0.36753041 0.39285714 0.35845588\n 0.54754098 0.         0.62663185 0.         0.         0.\n 0.84507042 0.         0.57692308 0.                nan 0.49045521\n 0.         0.         0.         0.54716981 0.         0.54545455]\nTrain Loss: 2.1185 Acc: 0.4511\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 70/70 [01:28<00:00,  1.27s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3333 Acc: 0.6411\nEpoch 2/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 278/278 [16:29<00:00,  3.56s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.99020032 0.96310708 0.95546909 0.98702983 0.99120911 0.9445165\n 0.99048854 0.99034443 0.99409137 0.90618245 0.96642167 0.96728635\n 0.96613345 0.99020032 0.96815103 0.99625306 0.99625306 0.99077677\n 0.99106499 0.99654129 0.9822741  0.98991209 0.99711774 0.95056925\n 0.9877504  0.98962387 0.99293846 0.9889033  0.98875919 0.98371523]\nrecall for all classes in train phase [0.         0.74458874 0.66528926 0.18867925 0.03333333 0.90510204\n 0.35227273 0.75714286 0.         0.85465116 0.52380952 0.74292453\n 0.8203125  0.28915663 0.82626263 0.         0.16666667 0.15277778\n 0.87951807 0.04       0.51515152 0.         0.         0.87809524\n 0.09090909 0.         0.05769231 0.73972603 0.19101124 0.56886228]\nprecision for all classes in train phase [0.         0.71369295 0.68656716 0.83333333 0.4        0.75233249\n 0.775      0.76258993        nan 0.63774403 0.53877551 0.72748268\n 0.74600355 0.72727273 0.75183824        nan 0.83333333 0.78571429\n 0.77659574 1.         0.6640625         nan        nan 0.62297297\n 0.61538462        nan 1.         0.73469388 0.73913043 0.69852941]\nTrain Loss: 1.1262 Acc: 0.6966\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 70/70 [01:25<00:00,  1.22s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.8947 Acc: 0.7537\nEpoch 3/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 278/278 [20:05<00:00,  4.34s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.99092088 0.97953596 0.97247442 0.99048854 0.99207379 0.95878369\n 0.99265024 0.99365903 0.99437959 0.94091368 0.97694192 0.9789595\n 0.9822741  0.99178556 0.97723015 0.9966854  0.99654129 0.99437959\n 0.99466782 0.99682951 0.98760628 0.99092088 0.99740597 0.96800692\n 0.99048854 0.99063266 0.99538838 0.99106499 0.99135322 0.98818274]\nrecall for all classes in train phase [0.11940299 0.83116883 0.79132231 0.55660377 0.2        0.93979592\n 0.57954545 0.87857143 0.04878049 0.89341085 0.67063492 0.84198113\n 0.89453125 0.57831325 0.85252525 0.11538462 0.26666667 0.56944444\n 0.92168675 0.12       0.75757576 0.15714286 0.1        0.87619048\n 0.54545455 0.11111111 0.38461538 0.78767123 0.57303371 0.75449102]\nprecision for all classes in train phase [0.66666667 0.85714286 0.80972516 0.75641026 0.63157895 0.80226481\n 0.78461538 0.82       1.         0.75450082 0.68699187 0.81880734\n 0.86907021 0.68571429 0.83234714 1.         0.8        0.83673469\n 0.86440678 1.         0.73099415 0.73333333 1.         0.74554295\n 0.64864865 0.88888889 1.         0.78767123 0.69863014 0.75449102]\nTrain Loss: 0.7469 Acc: 0.7916\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 70/70 [01:25<00:00,  1.23s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.6979 Acc: 0.8074\nEpoch 4/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 278/278 [16:23<00:00,  3.54s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.99077677 0.98616515 0.98025652 0.99164145 0.99452371 0.97088918\n 0.99452371 0.99625306 0.99466782 0.96382764 0.98429168 0.98515636\n 0.98616515 0.99423548 0.9844358  0.99682951 0.99697363 0.99610895\n 0.99740597 0.99783831 0.99164145 0.9922179  0.99798242 0.97939184\n 0.99135322 0.99077677 0.99769419 0.99466782 0.9955325  0.99120911]\nrecall for all classes in train phase [0.32835821 0.9047619  0.86157025 0.70754717 0.46666667 0.93979592\n 0.68181818 0.89285714 0.17073171 0.92635659 0.77380952 0.89622642\n 0.92382812 0.78313253 0.8969697  0.15384615 0.33333333 0.80555556\n 0.97590361 0.4        0.81818182 0.35714286 0.3        0.92\n 0.61363636 0.22222222 0.76923077 0.8630137  0.74157303 0.81437126]\nprecision for all classes in train phase [0.53658537 0.8893617  0.85626283 0.73529412 0.82352941 0.8656015\n 0.85714286 0.91911765 0.7        0.84526967 0.78947368 0.86560364\n 0.89245283 0.74712644 0.88622754 1.         0.90909091 0.81690141\n 0.92045455 1.         0.82822086 0.73529412 1.         0.82705479\n 0.675      0.66666667 0.90909091 0.88111888 0.89189189 0.81927711]\nTrain Loss: 0.5376 Acc: 0.8527\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 70/70 [01:22<00:00,  1.17s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.6102 Acc: 0.8366\nEpoch 5/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 278/278 [20:00<00:00,  4.32s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.99452371 0.99092088 0.98659749 0.99567661 0.99755008 0.97953596\n 0.9955325  0.99812653 0.99582072 0.9766537  0.98875919 0.98875919\n 0.99265024 0.99380314 0.98962387 0.99711774 0.99812653 0.99783831\n 0.99682951 0.9988471  0.99495605 0.99192967 0.99783831 0.98328289\n 0.99409137 0.9933708  0.99841476 0.99582072 0.9966854  0.99365903]\nrecall for all classes in train phase [0.64179104 0.93506494 0.88636364 0.82075472 0.81666667 0.9622449\n 0.77272727 0.95       0.36585366 0.94864341 0.86904762 0.92216981\n 0.95117188 0.72289157 0.91515152 0.26923077 0.63333333 0.93055556\n 0.97590361 0.68       0.89090909 0.4        0.3        0.92952381\n 0.72727273 0.51388889 0.84615385 0.89041096 0.84269663 0.85628743]\nprecision for all classes in train phase [0.75438596 0.92903226 0.91862955 0.8877551  0.89090909 0.89980916\n 0.86075949 0.95683453 0.83333333 0.89981618 0.82954545 0.89678899\n 0.94931774 0.75       0.9378882  0.875      0.9047619  0.87012987\n 0.9        1.         0.89634146 0.66666667 0.85714286 0.86067019\n 0.79012346 0.77083333 0.93617021 0.90909091 0.89285714 0.87730061]\nTrain Loss: 0.3887 Acc: 0.8967\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 70/70 [01:26<00:00,  1.23s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.5490 Acc: 0.8491\nEpoch 6/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 278/278 [16:17<00:00,  3.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.9955325  0.99437959 0.98702983 0.99783831 0.99711774 0.98717394\n 0.99755008 0.9988471  0.99711774 0.98198588 0.99207379 0.99178556\n 0.99293846 0.99654129 0.99308258 0.99827064 0.99855887 0.99855887\n 0.99812653 0.99870298 0.99639718 0.99365903 0.99841476 0.99149733\n 0.99596484 0.9955325  0.99870298 0.99798242 0.99769419 0.99538838]\nrecall for all classes in train phase [0.73134328 0.96320346 0.90909091 0.90566038 0.8        0.9744898\n 0.86363636 0.97142857 0.6097561  0.95445736 0.90079365 0.93632075\n 0.95703125 0.86746988 0.94747475 0.53846154 0.7        0.94444444\n 0.97590361 0.76       0.90909091 0.58571429 0.45       0.96190476\n 0.79545455 0.68055556 0.84615385 0.96575342 0.88764045 0.91017964]\nprecision for all classes in train phase [0.79032258 0.95289079 0.90534979 0.95049505 0.85714286 0.93719333\n 0.9382716  0.97142857 0.86206897 0.92662277 0.88326848 0.92974239\n 0.94777563 0.84705882 0.95519348 1.         0.95454545 0.91891892\n 0.94736842 0.86363636 0.9375     0.73214286 1.         0.92830882\n 0.875      0.85964912 0.97777778 0.94       0.92941176 0.89940828]\nTrain Loss: 0.2819 Acc: 0.9272\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 70/70 [01:32<00:00,  1.32s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.5141 Acc: 0.8674\nEpoch 7/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 278/278 [16:24<00:00,  3.54s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.9966854  0.99409137 0.99135322 0.99755008 0.99812653 0.98991209\n 0.99841476 0.99927944 0.99740597 0.98429168 0.99380314 0.99394725\n 0.9955325  0.99812653 0.99524427 0.99783831 0.99841476 0.99812653\n 0.99855887 0.99942355 0.99827064 0.99495605 0.99855887 0.99351492\n 0.9966854  0.99596484 0.99841476 0.99870298 0.99783831 0.99711774]\nrecall for all classes in train phase [0.80597015 0.95887446 0.94421488 0.9245283  0.86666667 0.9755102\n 0.92045455 0.98571429 0.68292683 0.95348837 0.93650794 0.95754717\n 0.97265625 0.95180723 0.96565657 0.57692308 0.7        0.91666667\n 0.98192771 0.84       0.95757576 0.6        0.6        0.96952381\n 0.82954545 0.79166667 0.82692308 0.97260274 0.8988764  0.92215569]\nprecision for all classes in train phase [0.84375    0.95268817 0.93265306 0.91588785 0.9122807  0.95409182\n 0.95294118 0.9787234  0.84848485 0.94162679 0.8973384  0.94418605\n 0.96699029 0.89772727 0.96761134 0.78947368 0.91304348 0.90410959\n 0.95882353 1.         0.96932515 0.85714286 0.85714286 0.94609665\n 0.90123457 0.81428571 0.95555556 0.96598639 0.93023256 0.95652174]\nTrain Loss: 0.2107 Acc: 0.9431\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 70/70 [01:28<00:00,  1.26s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.5345 Acc: 0.8623\nEpoch 8/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 278/278 [19:47<00:00,  4.27s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.9988471  0.99798242 0.99466782 0.99841476 0.99870298 0.99250612\n 0.99899121 0.99913532 0.99870298 0.99135322 0.99582072 0.99625306\n 0.99639718 0.9988471  0.99524427 0.99827064 0.99870298 0.99956766\n 1.         0.99870298 0.99870298 0.99682951 0.99927944 0.99610895\n 0.99783831 0.99755008 0.99956766 0.99827064 0.99899121 0.99855887]\nrecall for all classes in train phase [0.95522388 0.98917749 0.95661157 0.94339623 0.88333333 0.98061224\n 0.94318182 0.98571429 0.85365854 0.97674419 0.94444444 0.97641509\n 0.97851562 0.93975904 0.96767677 0.69230769 0.7        0.97222222\n 1.         0.88       0.97575758 0.77142857 0.8        0.98857143\n 0.88636364 0.83333333 0.94230769 0.96575342 0.93258427 0.97005988]\nprecision for all classes in train phase [0.92753623 0.9806867  0.96659708 0.95238095 0.96363636 0.9668008\n 0.97647059 0.97183099 0.92105263 0.96551724 0.94071146 0.9627907\n 0.97281553 0.96296296 0.96572581 0.81818182 1.         0.98591549\n 1.         0.78571429 0.96987952 0.9        0.94117647 0.96111111\n 0.93975904 0.92307692 1.         0.9527027  0.98809524 0.97005988]\nTrain Loss: 0.1493 Acc: 0.9644\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 70/70 [01:28<00:00,  1.27s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.5035 Acc: 0.8749\nEpoch 9/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 278/278 [20:28<00:00,  4.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.99812653 0.99654129 0.99524427 0.99899121 0.99870298 0.99164145\n 0.99870298 0.99956766 0.99841476 0.99092088 0.99639718 0.99697363\n 0.99682951 0.99827064 0.9966854  0.9988471  0.99899121 0.99956766\n 0.99899121 0.99985589 0.99726185 0.99755008 0.99985589 0.99625306\n 0.99870298 0.99841476 0.99927944 0.99927944 0.99942355 0.99913532]\nrecall for all classes in train phase [0.89552239 0.96753247 0.97107438 0.94339623 0.93333333 0.97755102\n 0.95454545 0.99285714 0.82926829 0.97577519 0.95238095 0.98113208\n 0.97851562 0.90361446 0.97171717 0.73076923 0.83333333 0.97222222\n 0.98795181 0.96       0.94545455 0.84285714 0.95       0.98095238\n 0.94318182 0.93055556 0.90384615 0.97945205 0.96629213 0.9760479 ]\nprecision for all classes in train phase [0.90909091 0.98026316 0.96114519 0.99009901 0.91803279 0.9637827\n 0.94382022 0.9858156  0.89473684 0.96363636 0.9486166  0.96969697\n 0.97851562 0.94936709 0.98163265 0.95       0.92592593 0.98591549\n 0.9704142  1.         0.93975904 0.90769231 1.         0.96986817\n 0.95402299 0.91780822 1.         0.9862069  0.98850575 0.98787879]\nTrain Loss: 0.1291 Acc: 0.9667\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 70/70 [01:24<00:00,  1.21s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.4783 Acc: 0.8851\nEpoch 10/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 278/278 [16:44<00:00,  3.61s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.99927944 0.99769419 0.99697363 0.9988471  0.99913532 0.99610895\n 0.99956766 0.9988471  0.99899121 0.99351492 0.99841476 0.99697363\n 0.99682951 0.99913532 0.99740597 0.99927944 0.99942355 0.99942355\n 0.99942355 0.99985589 0.99927944 0.99812653 0.99956766 0.99755008\n 0.99841476 0.99927944 0.99971177 0.9988471  0.9988471  0.99827064]\nrecall for all classes in train phase [0.97014925 0.98701299 0.97933884 0.96226415 0.91666667 0.99183673\n 0.98863636 0.98571429 0.87804878 0.97965116 0.98015873 0.97641509\n 0.98046875 0.96385542 0.97575758 0.88461538 0.86666667 0.94444444\n 0.98192771 0.96       0.98181818 0.88571429 0.9        0.98666667\n 0.90909091 0.95833333 0.96153846 0.97945205 0.95505618 0.9760479 ]\nprecision for all classes in train phase [0.95588235 0.97854077 0.97731959 0.96226415 0.98214286 0.98082745\n 0.97752809 0.95833333 0.94736842 0.97681159 0.97628458 0.97411765\n 0.9766537  0.96385542 0.98773006 0.92       1.         1.\n 0.99390244 1.         0.98780488 0.92537313 0.94736842 0.98106061\n 0.96385542 0.97183099 1.         0.96621622 0.95505618 0.95321637]\nTrain Loss: 0.1012 Acc: 0.9765\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 70/70 [01:31<00:00,  1.30s/it]","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.4702 Acc: 0.8851\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}