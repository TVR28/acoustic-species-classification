{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7149813,"sourceType":"datasetVersion","datasetId":4127981}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install split-folders","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-07T23:23:13.052132Z","iopub.execute_input":"2023-12-07T23:23:13.052842Z","iopub.status.idle":"2023-12-07T23:23:26.273632Z","shell.execute_reply.started":"2023-12-07T23:23:13.052814Z","shell.execute_reply":"2023-12-07T23:23:26.272641Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting split-folders\n  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\nInstalling collected packages: split-folders\nSuccessfully installed split-folders-0.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install efficientnet_pytorch","metadata":{"execution":{"iopub.status.busy":"2023-12-07T23:23:32.316970Z","iopub.execute_input":"2023-12-07T23:23:32.317328Z","iopub.status.idle":"2023-12-07T23:23:46.579524Z","shell.execute_reply.started":"2023-12-07T23:23:32.317298Z","shell.execute_reply":"2023-12-07T23:23:46.578626Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting efficientnet_pytorch\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet_pytorch) (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\nBuilding wheels for collected packages: efficientnet_pytorch\n  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=d53c18e40bc8c4495380ed64dd352f420a512fb00b2cd2f8082b7616a116d062\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\nSuccessfully built efficientnet_pytorch\nInstalling collected packages: efficientnet_pytorch\nSuccessfully installed efficientnet_pytorch-0.7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision.transforms import transforms\nimport os\nfrom PIL import Image\nimport torch\nimport ssl\nimport torchvision\nimport torchvision.models as models\nimport torch.optim as optim\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport splitfolders\nimport csv\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\nimport numpy as np\nfrom efficientnet_pytorch import EfficientNet\nimport warnings\n\nwarnings.filterwarnings(\n    action='ignore')\n\n\ndef calculate_accuracy(y_pred, y):\n    top_pred = y_pred.argmax(1, keepdim=True)\n    correct = top_pred.eq(y.view_as(top_pred)).sum()\n    acc = correct.float() / y.shape[0]\n    return acc\n\nssl._create_default_https_context = ssl._create_unverified_context\n\nif __name__ == '__main__':\n    # Define the device to be used for training\n    device = torch.device(\"cpu\")\n\n    # Set up the transform to resize and normalize the images\n    transform = transforms.Compose([\n        transforms.Resize(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n            std=[0.5, 0.5, 0.5]),\n    ])\n     \n   # loading the temp.zip and creating a zip object \n  # with ZipFile(\"C:\\\\Users\\\\sai mohan pulamolu\\\\\\ \n   #         Desktop\\\\geeks_dir\\\\temp\\\\temp.zip\", 'r') as zObject: \n  \n    # Extracting all the members of the zip  \n    # into a specific location. \n   # zObject.extractall( \n    #    path=\"C:\\\\Users\\\\sai mohan pulamolu\\\\Desktop\\\\geeks_dir\\\\temp\") \n    # Update input folder and output folder paths\n    input_folder = r\"/kaggle/input/base-dataset-without-preprocessing/Without_any_preprocessing_small\"\n    output_folder = r\"/kaggle/working/images_30_output\"\n    \n\n    ### Uncomment only for first time. once data is splitted into train and validation, comment it out\n    splitfolders.ratio(input_folder, output_folder, seed=42, ratio=(0.8, 0.2), group_prefix=None)\n\n    # Create datasets for the training and testing sets\n    train_dataset = torchvision.datasets.ImageFolder(output_folder + '/train', transform=transform)\n    val_dataset = torchvision.datasets.ImageFolder(output_folder + '/val', transform=transform)\n    train_size = len(train_dataset)\n    val_size = len(val_dataset)\n\n    # Create the data loaders for training and validation\n    train_loader = DataLoader(train_dataset, batch_size=25, shuffle=True,num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=25, shuffle=True,num_workers=4)\n    list_of_classes = os.listdir(r\"/kaggle/working/images_30_output/train\")\n    print(list_of_classes)\n    classes = list(train_dataset.class_to_idx.keys())\n    classes.sort()\n\n\n    # Define the VGG model\n    model_name = 'efficientnet-b0'  # You can change this to other versions (b1, b2, etc.)\n    model = EfficientNet.from_pretrained(model_name)\n    num_features = model._fc.in_features\n    model._fc = nn.Linear(num_features, len(list_of_classes))\n\n  \n\n    ## uncomment for CPU\n    model = model.to(device)\n\n    # Define the loss function and optimizer\n    \n    \n    criterion = nn.CrossEntropyLoss()\n    START_LR = 0.0001\n    optimizer = optim.Adam(model.parameters(), lr=START_LR)\n    model = model.to(device)\n    criterion = criterion.to(device)\n\n    ### uncomment for GPU\n    #model.cuda()\n        # Train the model\n    for epoch in range(10):\n        ### uncomment for GPU\n        #torch.cuda.empty_cache()\n        print('Epoch {}/{}'.format(epoch + 1, 10))\n        print('-' * 10)\n\n        running_loss = 0\n        running_corrects = 0\n\n        model.train()\n        predictions = []\n        true_labels = []\n        for inputs, labels in tqdm(train_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            true_labels.extend(labels.cpu().numpy())\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            predictions.extend(preds.cpu().numpy())\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n        epoch_loss = running_loss / train_size\n        epoch_acc = running_corrects.double() / train_size\n        report_dict = classification_report(true_labels, predictions, target_names=list_of_classes,output_dict=True)\n        report_pd = pd.DataFrame(report_dict)\n        report_pd.to_csv('training-classification-epoch' + str(epoch + 1) + '.csv')\n        cnf_matrix = confusion_matrix(true_labels, predictions)\n        df_cm = pd.DataFrame(cnf_matrix / np.sum(cnf_matrix, axis=1)[:, None], index = [i for i in classes],\n                        columns = [i for i in classes])\n        df_cm.to_csv('confusion-matrix-train-epoch' + str(epoch + 1) + '.csv')\n        #acc = matrix.diagonal()/matrix.sum(axis=1)\n        FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n        FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n        TP = np.diag(cnf_matrix)\n        TN = cnf_matrix.sum() - (FP + FN + TP)\n        FP = FP.astype(float)\n        FN = FN.astype(float)\n        TP = TP.astype(float)\n        TN = TN.astype(float)\n        ACC = (TP+TN)/(TP+FP+FN+TN)\n        TPR = TP/(TP+FN)\n        PPV = TP/(TP+FP)\n        print(\"accuracy for all classes in train phase\", ACC)\n        print(\"recall for all classes in train phase\", TPR)\n        print(\"precision for all classes in train phase\", PPV)\n        \n        pd.DataFrame(ACC, columns=['Accuracy']).to_csv('accuracy-train-epoch' + str(epoch + 1) + '.csv')\n        print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n        \n        # Delete data to clear GPU memory\n        del outputs\n        del preds\n        del labels\n        del inputs\n        torch.cuda.empty_cache()\n        \n        # Validation phase\n        running_loss = 0\n        running_corrects = 0\n        model.eval()  # set the model to evaluation mode\n        predictions = []\n        true_labels = []\n        for inputs, labels in tqdm(val_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            true_labels.extend(labels.cpu().numpy())\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            predictions.extend(preds.cpu().numpy())\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n        epoch_loss = running_loss / val_size\n        epoch_acc = running_corrects.double() / val_size\n       \n        #classification_report(true_labels, predictions, target_names=list_of_classes,output_dict=True)\n        report_dict = classification_report(true_labels, predictions, target_names=list_of_classes,output_dict=True)\n        report_pd = pd.DataFrame(report_dict)\n        report_pd.to_csv('val-classification-epoch' + str(epoch + 1) + '.csv')\n        #matrix = confusion_matrix(true_labels, predictions)\n        cnf_matrix = confusion_matrix(true_labels, predictions)\n        df_cm = pd.DataFrame(cnf_matrix / np.sum(cnf_matrix, axis=1)[:, None], index = [i for i in classes],\n                        columns = [i for i in classes])\n        df_cm.to_csv('confusion-matrix-val-epoch' + str(epoch + 1) + '.csv')\n        #acc = matrix.diagonal()/matrix.sum(axis=1)\n        FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n        FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n        TP = np.diag(cnf_matrix)\n        TN = cnf_matrix.sum() - (FP + FN + TP)\n        FP = FP.astype(float)\n        FN = FN.astype(float)\n        TP = TP.astype(float)\n        TN = TN.astype(float)\n        ACC = (TP+TN)/(TP+FP+FN+TN)\n        TPR = TP/(TP+FN)\n        PPV = TP/(TP+FP)\n        pd.DataFrame(ACC, columns=['Accuracy']).to_csv('accuracy-val-epoch' + str(epoch + 1) + '.csv')\n        # print(\"accuracy for all classes in validation phase\", ACC)\n        # print(\"recall for all classes in validation phase\", TPR)\n        # print(\"precision for all classes in validation phase\", PPV)\n        print('Val Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n        \n        # Delete data to clear GPU memory\n        del outputs\n        del preds\n        del labels\n        del inputs\n        torch.cuda.empty_cache()\n                \n        \n\n    # Save the model\n    torch.save(model, 'efficientnet_model.pth')","metadata":{"execution":{"iopub.status.busy":"2023-12-07T23:24:13.254213Z","iopub.execute_input":"2023-12-07T23:24:13.254740Z","iopub.status.idle":"2023-12-08T00:11:52.816655Z","shell.execute_reply.started":"2023-12-07T23:24:13.254692Z","shell.execute_reply":"2023-12-08T00:11:52.815471Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nCopying files: 2225 files [00:16, 137.57 files/s]\nDownloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n","output_type":"stream"},{"name":"stdout","text":"['blakit1', 'abhori1', 'chespa1', 'whctur2', 'whbcan1', 'combul2', 'somgre1', 'spwlap1', 'gobwea1', 'yebsto1', 'chtapa3', 'vilwea1', 'reccuc1', 'whbtit5', 'nubwoo1', 'slbgre1', 'spewea1', 'colsun2', 'gobbun1', 'norpuf1', 'brican1', 'pygbat1', 'grbcam1', 'palfly2', 'carcha1', 'rebfir2', 'afrjac1', 'chibat1', 'afghor1', 'grewoo2']\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20.4M/20.4M [00:00<00:00, 206MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b0\nEpoch 1/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 71/71 [05:09<00:00,  4.36s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.93947964 0.95984163 0.98585973 0.80090498 0.96719457 0.85011312\n 0.9841629  0.96040724 0.9841629  0.91515837 0.6408371  0.94230769\n 0.99547511 0.93099548 0.94852941 0.989819   0.98868778 0.99321267\n 0.99377828 0.9790724  0.91346154 0.98585973 0.84276018 0.97850679\n 0.96776018 0.94626697 0.99377828 0.98925339 0.99717195 0.99717195]\nrecall for all classes in train phase [0.03       0.03508772 0.         0.37799043 0.         0.32786885\n 0.         0.01587302 0.         0.125      0.47863248 0.078125\n 0.         0.02666667 0.04878049 0.         0.         0.\n 0.         0.         0.09278351 0.         0.24528302 0.04\n 0.04255319 0.04285714 0.         0.         0.         0.        ]\nprecision for all classes in train phase [0.23076923 0.11111111 0.         0.26245847 0.         0.1793722\n 0.         0.11111111 0.         0.42857143 0.1792     0.10416667\n 0.         0.03921569 0.23529412 0.         0.         0.\n        nan        nan 0.12162162 0.         0.19796954 0.06666667\n 0.14285714 0.09677419 0.         0.         0.         0.        ]\nTrain Loss: 3.0851 Acc: 0.1810\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 19/19 [00:26<00:00,  1.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.0863 Acc: 0.1838\nEpoch 2/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 71/71 [04:15<00:00,  3.60s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.9428733  0.96945701 0.98585973 0.80825792 0.98699095 0.90045249\n 0.99773756 0.96436652 0.98812217 0.91572398 0.70871041 0.96719457\n 0.99773756 0.95701357 0.95984163 0.99095023 0.99038462 0.99377828\n 0.99321267 0.9790724  0.94061086 0.989819   0.90497738 0.98585973\n 0.97002262 0.95644796 0.99547511 0.989819   0.99773756 0.99773756]\nrecall for all classes in train phase [0.09       0.07017544 0.         0.81339713 0.         0.59016393\n 0.         0.         0.         0.38888889 0.73931624 0.140625\n 0.         0.01333333 0.26829268 0.         0.         0.\n 0.         0.         0.45360825 0.         0.33333333 0.\n 0.34042553 0.18571429 0.         0.         0.         0.        ]\nprecision for all classes in train phase [0.47368421 0.8        0.         0.36170213        nan 0.36363636\n        nan        nan        nan 0.47863248 0.27591707 0.75\n        nan 0.33333333 0.66666667        nan        nan        nan\n 0.                nan 0.45833333        nan 0.46086957        nan\n 0.42105263 0.39393939        nan        nan        nan        nan]\nTrain Loss: 2.3802 Acc: 0.3631\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 19/19 [00:27<00:00,  1.45s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.3554 Acc: 0.3239\nEpoch 3/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 71/71 [04:09<00:00,  3.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.9428733  0.97115385 0.98642534 0.9111991  0.98699095 0.93665158\n 0.99773756 0.96493213 0.98812217 0.93269231 0.84502262 0.97737557\n 0.99773756 0.95418552 0.94739819 0.99095023 0.99038462 0.99377828\n 0.99377828 0.9790724  0.95701357 0.989819   0.92816742 0.98585973\n 0.97624434 0.95361991 0.99547511 0.989819   0.99773756 0.99773756]\nrecall for all classes in train phase [0.42       0.22807018 0.         0.86602871 0.         0.70491803\n 0.         0.01587302 0.         0.63194444 0.8034188  0.609375\n 0.         0.22666667 0.57317073 0.         0.         0.\n 0.         0.         0.69072165 0.         0.70440252 0.\n 0.46808511 0.44285714 0.         0.         0.         0.        ]\nprecision for all classes in train phase [0.49411765 0.65              nan 0.58387097        nan 0.5308642\n        nan 1.                nan 0.57961783 0.45192308 0.72222222\n        nan 0.425      0.44761905        nan        nan        nan\n        nan        nan 0.59292035        nan 0.58333333        nan\n 0.56410256 0.41891892        nan        nan        nan        nan]\nTrain Loss: 1.7865 Acc: 0.5300\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 19/19 [00:25<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.7572 Acc: 0.5274\nEpoch 4/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 71/71 [04:12<00:00,  3.56s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.96606335 0.97624434 0.98642534 0.94513575 0.98755656 0.95418552\n 0.99773756 0.96945701 0.98868778 0.94513575 0.91459276 0.98020362\n 0.99773756 0.9581448  0.96040724 0.99095023 0.99038462 0.99377828\n 0.99434389 0.9790724  0.9688914  0.99038462 0.95079186 0.98585973\n 0.97511312 0.95927602 0.99660633 0.989819   0.99773756 0.99886878]\nrecall for all classes in train phase [0.69       0.57894737 0.         0.88516746 0.04347826 0.76229508\n 0.         0.19047619 0.04761905 0.83333333 0.88034188 0.6875\n 0.         0.42666667 0.74390244 0.         0.         0.\n 0.09090909 0.         0.8556701  0.05555556 0.82389937 0.\n 0.59574468 0.5        0.25       0.         0.         0.5       ]\nprecision for all classes in train phase [0.70408163 0.64705882        nan 0.71705426 1.         0.64137931\n        nan 0.8        1.         0.62176166 0.62613982 0.74576271\n        nan 0.50793651 0.55454545        nan        nan        nan\n 1.                nan 0.66935484 1.         0.68947368        nan\n 0.52830189 0.48611111 1.                nan        nan 1.        ]\nTrain Loss: 1.3558 Acc: 0.6448\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 19/19 [00:26<00:00,  1.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4767 Acc: 0.5864\nEpoch 5/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 71/71 [04:09<00:00,  3.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.97794118 0.98303167 0.98868778 0.95644796 0.98925339 0.96945701\n 0.99773756 0.97171946 0.989819   0.9581448  0.94174208 0.97737557\n 0.99773756 0.95757919 0.96436652 0.99095023 0.99038462 0.99377828\n 0.99377828 0.97963801 0.98359729 0.99151584 0.95871041 0.98642534\n 0.98133484 0.97002262 0.99547511 0.99038462 0.99886878 0.99830317]\nrecall for all classes in train phase [0.91       0.68421053 0.16666667 0.93301435 0.17391304 0.81147541\n 0.         0.36507937 0.14285714 0.86805556 0.87606838 0.75\n 0.         0.61333333 0.76829268 0.         0.         0.\n 0.         0.08108108 0.89690722 0.16666667 0.87421384 0.04\n 0.70212766 0.62857143 0.         0.05555556 0.5        0.25      ]\nprecision for all classes in train phase [0.75206612 0.76470588 1.         0.75581395 1.         0.76153846\n        nan 0.6969697  1.         0.69444444 0.73476703 0.66666667\n        nan 0.5        0.58878505        nan        nan        nan\n        nan 0.6        0.82075472 1.         0.72395833 1.\n 0.63461538 0.61971831        nan 1.         1.         1.        ]\nTrain Loss: 1.0580 Acc: 0.7121\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 19/19 [00:26<00:00,  1.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.2998 Acc: 0.6389\nEpoch 6/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 71/71 [04:08<00:00,  3.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.98246606 0.98246606 0.99095023 0.97511312 0.99151584 0.97567873\n 0.99717195 0.9790724  0.99208145 0.97058824 0.96266968 0.98642534\n 0.99773756 0.96549774 0.97624434 0.99095023 0.99208145 0.99377828\n 0.9949095  0.98246606 0.98699095 0.99208145 0.98133484 0.98868778\n 0.98868778 0.9739819  0.99604072 0.989819   0.99943439 1.        ]\nrecall for all classes in train phase [0.92       0.78947368 0.45833333 0.9569378  0.52173913 0.85245902\n 0.         0.6031746  0.38095238 0.9375     0.93162393 0.859375\n 0.         0.70666667 0.82926829 0.         0.17647059 0.\n 0.18181818 0.2972973  0.87628866 0.22222222 0.91823899 0.2\n 0.82978723 0.74285714 0.125      0.         0.75       1.        ]\nprecision for all classes in train phase [0.8        0.703125   0.78571429 0.85106383 0.75       0.80620155\n 0.         0.76       0.88888889 0.75842697 0.81343284 0.78571429\n        nan 0.57608696 0.70833333        nan 1.                nan\n 1.         0.6875     0.88541667 1.         0.87951807 1.\n 0.76470588 0.65       1.                nan 1.         1.        ]\nTrain Loss: 0.8348 Acc: 0.7885\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 19/19 [00:27<00:00,  1.44s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.2053 Acc: 0.6849\nEpoch 7/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 71/71 [04:14<00:00,  3.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.98699095 0.99151584 0.99151584 0.97963801 0.98868778 0.98585973\n 0.99773756 0.98303167 0.99547511 0.98076923 0.97511312 0.98925339\n 0.99773756 0.97850679 0.98246606 0.99095023 0.99377828 0.99434389\n 0.99547511 0.98585973 0.99038462 0.99321267 0.98699095 0.98812217\n 0.99208145 0.97794118 0.99773756 0.989819   0.99943439 0.99943439]\nrecall for all classes in train phase [0.95       0.85964912 0.54166667 0.97129187 0.52173913 0.89344262\n 0.25       0.68253968 0.61904762 0.97222222 0.94444444 0.875\n 0.         0.89333333 0.90243902 0.         0.35294118 0.09090909\n 0.27272727 0.45945946 0.94845361 0.33333333 0.96855346 0.16\n 0.87234043 0.78571429 0.5        0.         0.75       0.75      ]\nprecision for all classes in train phase [0.84070796 0.875      0.76470588 0.87124464 0.57142857 0.90082645\n 0.5        0.81132075 1.         0.82352941 0.87698413 0.8358209\n        nan 0.69072165 0.7628866         nan 1.         1.\n 1.         0.77272727 0.88461538 1.         0.89534884 1.\n 0.83673469 0.69620253 1.                nan 1.         1.        ]\nTrain Loss: 0.6551 Acc: 0.8399\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 19/19 [00:25<00:00,  1.34s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.1151 Acc: 0.6980\nEpoch 8/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 71/71 [04:09<00:00,  3.51s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.98925339 0.99547511 0.99377828 0.98642534 0.99038462 0.98190045\n 0.99886878 0.98585973 0.99717195 0.98585973 0.97794118 0.99208145\n 0.99830317 0.98529412 0.98868778 0.99095023 0.9949095  0.99434389\n 0.99660633 0.98585973 0.99321267 0.99321267 0.98925339 0.99208145\n 0.99321267 0.98246606 0.99660633 0.99095023 0.99886878 1.        ]\nrecall for all classes in train phase [0.94       0.9122807  0.66666667 0.97607656 0.60869565 0.93442623\n 0.5        0.71428571 0.80952381 0.95138889 0.96153846 0.90625\n 0.25       0.88       0.95121951 0.0625     0.52941176 0.09090909\n 0.45454545 0.64864865 0.93814433 0.38888889 0.95597484 0.44\n 0.91489362 0.85714286 0.25       0.11111111 0.75       1.        ]\nprecision for all classes in train phase [0.87850467 0.94545455 0.84210526 0.91479821 0.63636364 0.82608696\n 1.         0.86538462 0.94444444 0.88387097 0.88235294 0.87878788\n 1.         0.79518072 0.82978723 0.5        0.9        1.\n 1.         0.66666667 0.93814433 0.875      0.92682927 1.\n 0.84313725 0.74074074 1.         1.         0.75       1.        ]\nTrain Loss: 0.5176 Acc: 0.8699\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 19/19 [00:26<00:00,  1.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.0861 Acc: 0.7221\nEpoch 9/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 71/71 [04:13<00:00,  3.57s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.99434389 0.99377828 0.9949095  0.9841629  0.99208145 0.98868778\n 0.99943439 0.989819   0.99604072 0.98755656 0.98246606 0.99547511\n 0.99773756 0.98472851 0.99208145 0.99321267 0.99377828 0.9949095\n 0.99773756 0.98925339 0.99377828 0.99547511 0.99377828 0.99377828\n 0.99604072 0.98868778 0.99717195 0.99321267 1.         1.        ]\nrecall for all classes in train phase [0.98       0.92982456 0.75       0.96172249 0.86956522 0.92622951\n 0.75       0.79365079 0.71428571 0.97222222 0.97008547 0.9375\n 0.         0.96       0.96341463 0.375      0.52941176 0.18181818\n 0.72727273 0.54054054 0.93814433 0.55555556 0.98113208 0.56\n 0.93617021 0.85714286 0.375      0.33333333 1.         1.        ]\nprecision for all classes in train phase [0.9245283  0.88333333 0.85714286 0.90950226 0.64516129 0.91129032\n 1.         0.90909091 0.9375     0.88607595 0.90438247 0.9375\n        nan 0.75       0.87777778 0.75       0.75       1.\n 0.88888889 0.90909091 0.94791667 1.         0.95121951 1.\n 0.91666667 0.85714286 1.         1.         1.         1.        ]\nTrain Loss: 0.4201 Acc: 0.8971\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 19/19 [00:25<00:00,  1.35s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.0694 Acc: 0.7177\nEpoch 10/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 71/71 [04:10<00:00,  3.53s/it]\n","output_type":"stream"},{"name":"stdout","text":"accuracy for all classes in train phase [0.99208145 0.99660633 0.9949095  0.99321267 0.99434389 0.99208145\n 0.99886878 0.99151584 0.99773756 0.99264706 0.98642534 0.99660633\n 0.99773756 0.99434389 0.99547511 0.99434389 0.99660633 0.99434389\n 0.99717195 0.99264706 0.99773756 0.99660633 0.99434389 0.99660633\n 0.99547511 0.99377828 0.99943439 0.99547511 0.99886878 0.99943439]\nrecall for all classes in train phase [0.95       0.92982456 0.83333333 0.98564593 0.86956522 0.95901639\n 0.5        0.88888889 0.9047619  0.99305556 0.96581197 0.9375\n 0.         0.94666667 0.98780488 0.5        0.76470588 0.09090909\n 0.81818182 0.81081081 0.97938144 0.66666667 0.98113208 0.76\n 0.89361702 0.92857143 0.875      0.55555556 0.75       0.75      ]\nprecision for all classes in train phase [0.91346154 0.96363636 0.8        0.95813953 0.74074074 0.92857143\n 1.         0.875      0.9047619  0.92258065 0.9338843  0.96774194\n        nan 0.92207792 0.92045455 0.8        0.86666667 1.\n 0.75       0.83333333 0.97938144 1.         0.95705521 1.\n 0.93333333 0.91549296 1.         1.         0.75       1.        ]\nTrain Loss: 0.3371 Acc: 0.9287\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 19/19 [00:25<00:00,  1.36s/it]","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.0194 Acc: 0.7309\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}